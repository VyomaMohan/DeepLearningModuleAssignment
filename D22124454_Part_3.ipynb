{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea43bb56",
   "metadata": {
    "id": "ea43bb56"
   },
   "source": [
    "# D22124454 - Deep Learning Assignment - Part 3\n",
    "# Writing news articles:\n",
    "\n",
    "#### System specifics:\n",
    "OS: Windows 11\n",
    "\n",
    "RAM: 32 GB\n",
    "\n",
    "GPU: RTX 3070\n",
    "\n",
    "IDE: Models initially trained and evaluated on local - Jupyter IDE\n",
    "\n",
    "#### Task overview:\n",
    "This task is about trying to generate news articles from the given data using the text portion. We will be using the articles from the top 2 categories of articles.\n",
    "\n",
    "#### Note:\n",
    "#### In this notebook, we have the training and validation comparisons, along with a graph for evaluation. The best performing models will be evaluated again in the demo notebooks. The training was done in local with 10 epochs, so it might not give similar results with less epochs in collab.\n",
    "\n",
    "#### FAQ:\n",
    "- If some of the plotly graphs are not rendering, run the code above the imports. For local running, it is \"iframe\". For colab running, it is \"colab\".\n",
    "- Please enable GPU before running\n",
    "\n",
    "### Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9pl97hgfN2-k",
   "metadata": {
    "id": "9pl97hgfN2-k"
   },
   "outputs": [],
   "source": [
    "#IF PLOTLY GRAPHS DO NOT RENDER. SET TO COLAB WHEN RUN IN COLAB AND IFRAME FOR LOCAL (iframe)\n",
    "#import plotly.io as pio\n",
    "#pio.renderers.default = 'colab'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "baa895d6",
   "metadata": {
    "id": "baa895d6"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d52c3d04",
   "metadata": {
    "id": "d52c3d04"
   },
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Embedding, Conv1D, GlobalMaxPooling1D, GlobalAveragePooling1D, LSTM, SimpleRNN, MaxPooling1D, Conv1D, TimeDistributed, AveragePooling1D, Input\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.datasets import imdb\n",
    "import tensorflow as tf\n",
    "\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "797793d3",
   "metadata": {
    "id": "797793d3"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import re\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7xK0z_YSNWag",
   "metadata": {
    "id": "7xK0z_YSNWag"
   },
   "outputs": [],
   "source": [
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "lzfqboJxNbRz",
   "metadata": {
    "id": "lzfqboJxNbRz"
   },
   "outputs": [],
   "source": [
    "# credentials to get the data file\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25eb27a3",
   "metadata": {
    "id": "25eb27a3"
   },
   "source": [
    "### Import the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "TVe9op5ANhcG",
   "metadata": {
    "id": "TVe9op5ANhcG"
   },
   "outputs": [],
   "source": [
    "# https://drive.google.com/file/d/1sD2qf_JAOXKPQ6VuxpprYGfAZr1dAWmY/view?usp=sharing\n",
    "bbcCsv = drive.CreateFile({'id':'1sD2qf_JAOXKPQ6VuxpprYGfAZr1dAWmY'})\n",
    "bbcCsv.GetContentFile('bbc-text.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f589ae0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "9f589ae0",
    "outputId": "f5fa1b30-363a-4a73-e220-976224267e47"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-7dd58e44-3589-408e-9924-78b436c034f9\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tech</td>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sport</td>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sport</td>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2220</th>\n",
       "      <td>business</td>\n",
       "      <td>cars pull down us retail figures us retail sal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2221</th>\n",
       "      <td>politics</td>\n",
       "      <td>kilroy unveils immigration policy ex-chatshow ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2222</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>rem announce new glasgow concert us band rem h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2223</th>\n",
       "      <td>politics</td>\n",
       "      <td>how political squabbles snowball it s become c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2224</th>\n",
       "      <td>sport</td>\n",
       "      <td>souness delight at euro progress boss graeme s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2225 rows × 2 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7dd58e44-3589-408e-9924-78b436c034f9')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-7dd58e44-3589-408e-9924-78b436c034f9 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-7dd58e44-3589-408e-9924-78b436c034f9');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "           category                                               text\n",
       "0              tech  tv future in the hands of viewers with home th...\n",
       "1          business  worldcom boss  left books alone  former worldc...\n",
       "2             sport  tigers wary of farrell  gamble  leicester say ...\n",
       "3             sport  yeading face newcastle in fa cup premiership s...\n",
       "4     entertainment  ocean s twelve raids box office ocean s twelve...\n",
       "...             ...                                                ...\n",
       "2220       business  cars pull down us retail figures us retail sal...\n",
       "2221       politics  kilroy unveils immigration policy ex-chatshow ...\n",
       "2222  entertainment  rem announce new glasgow concert us band rem h...\n",
       "2223       politics  how political squabbles snowball it s become c...\n",
       "2224          sport  souness delight at euro progress boss graeme s...\n",
       "\n",
       "[2225 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_data = pd.read_csv(\"bbc-text.csv\", delimiter =\",\", index_col=False)\n",
    "display(raw_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de5ec72",
   "metadata": {
    "id": "5de5ec72"
   },
   "source": [
    "### Identify the top 2 common categories and filter the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a03577",
   "metadata": {
    "id": "b8a03577"
   },
   "outputs": [],
   "source": [
    "topic_list_all = np.array(raw_data[\"category\"])\n",
    "unique_top, top_counts = np.unique(topic_list_all, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89c1e92",
   "metadata": {
    "id": "d89c1e92",
    "outputId": "6c214779-99b9-404a-d91c-15c7280a7507"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"545px\"\n",
       "    src=\"iframe_figures/figure_68.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = px.bar(\n",
    "    raw_data, x=unique_top, y=top_counts,\n",
    "    title = \"Fig: Topics in raw data\",\n",
    "    labels={'x' : 'Topics', 'y': 'No. of occurance'}\n",
    ")\n",
    "\n",
    "fig.update_layout(barmode='stack', xaxis={'categoryorder':'total descending'})\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e022e21d",
   "metadata": {
    "id": "e022e21d",
    "outputId": "6ec61ba8-d409-4527-81de-2b809ebd61ef"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sport</td>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sport</td>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sport</td>\n",
       "      <td>henman hopes ended in dubai third seed tim hen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sport</td>\n",
       "      <td>wilkinson fit to face edinburgh england captai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2214</th>\n",
       "      <td>business</td>\n",
       "      <td>bush budget seeks deep cutbacks president bush...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2218</th>\n",
       "      <td>sport</td>\n",
       "      <td>davies favours gloucester future wales hooker ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2219</th>\n",
       "      <td>business</td>\n",
       "      <td>beijingers fume over parking fees choking traf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2220</th>\n",
       "      <td>business</td>\n",
       "      <td>cars pull down us retail figures us retail sal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2224</th>\n",
       "      <td>sport</td>\n",
       "      <td>souness delight at euro progress boss graeme s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1021 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      category                                               text\n",
       "1     business  worldcom boss  left books alone  former worldc...\n",
       "2        sport  tigers wary of farrell  gamble  leicester say ...\n",
       "3        sport  yeading face newcastle in fa cup premiership s...\n",
       "7        sport  henman hopes ended in dubai third seed tim hen...\n",
       "8        sport  wilkinson fit to face edinburgh england captai...\n",
       "...        ...                                                ...\n",
       "2214  business  bush budget seeks deep cutbacks president bush...\n",
       "2218     sport  davies favours gloucester future wales hooker ...\n",
       "2219  business  beijingers fume over parking fees choking traf...\n",
       "2220  business  cars pull down us retail figures us retail sal...\n",
       "2224     sport  souness delight at euro progress boss graeme s...\n",
       "\n",
       "[1021 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "top_2_sections = [\"sport\", \"business\"]\n",
    "filtered_df = raw_data.loc[raw_data['category'].isin(top_2_sections)]\n",
    "display(filtered_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7c5bbe",
   "metadata": {
    "id": "0c7c5bbe"
   },
   "source": [
    "### Sample and clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646e2cf5",
   "metadata": {
    "id": "646e2cf5"
   },
   "outputs": [],
   "source": [
    "# Sample the data from the dataset\n",
    "sample_portion = filtered_df.sample(frac = 0.40, random_state= 10)\n",
    "\n",
    "#Clear the non-alphanumeric letters (like a-hat)\n",
    "sample_portion['text'] = sample_portion['text'].apply(lambda a: str(a).encode('ascii','ignore'))\n",
    "sample_portion['text'] = sample_portion['text'].apply(lambda a: a.decode('ascii','ignore'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d01147",
   "metadata": {
    "id": "b4d01147",
    "outputId": "2b19324f-9ee8-4d46-a1f0-c256aadf265d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "      <th>NoPunct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2187</th>\n",
       "      <td>sport</td>\n",
       "      <td>jones files conte lawsuit marion jones has fil...</td>\n",
       "      <td>jones files conte lawsuit marion jones has fil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1202</th>\n",
       "      <td>sport</td>\n",
       "      <td>what now for british tennis  tim henman s deci...</td>\n",
       "      <td>what now for british tennis  tim henman s deci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>868</th>\n",
       "      <td>sport</td>\n",
       "      <td>moody joins up with england lewis moody has fl...</td>\n",
       "      <td>moody joins up with england lewis moody has fl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1339</th>\n",
       "      <td>business</td>\n",
       "      <td>latin america sees strong growth latin america...</td>\n",
       "      <td>latin america sees strong growth latin america...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2077</th>\n",
       "      <td>sport</td>\n",
       "      <td>holmes starts 2005 with gb events kelly holmes...</td>\n",
       "      <td>holmes starts 2005 with gb events kelly holmes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>business</td>\n",
       "      <td>industrial output falls in japan japanese indu...</td>\n",
       "      <td>industrial output falls in japan japanese indu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1210</th>\n",
       "      <td>business</td>\n",
       "      <td>us adds more jobs than expected the us economy...</td>\n",
       "      <td>us adds more jobs than expected the us economy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577</th>\n",
       "      <td>sport</td>\n",
       "      <td>hearts of oak 3-2 cotonsport hearts of oak set...</td>\n",
       "      <td>hearts of oak 32 cotonsport hearts of oak set ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>810</th>\n",
       "      <td>sport</td>\n",
       "      <td>ronaldo considering new contract manchester un...</td>\n",
       "      <td>ronaldo considering new contract manchester un...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558</th>\n",
       "      <td>sport</td>\n",
       "      <td>mcilroy continues winning streak james mcilroy...</td>\n",
       "      <td>mcilroy continues winning streak james mcilroy...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>408 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      category                                               text   \n",
       "2187     sport  jones files conte lawsuit marion jones has fil...  \\\n",
       "1202     sport  what now for british tennis  tim henman s deci...   \n",
       "868      sport  moody joins up with england lewis moody has fl...   \n",
       "1339  business  latin america sees strong growth latin america...   \n",
       "2077     sport  holmes starts 2005 with gb events kelly holmes...   \n",
       "...        ...                                                ...   \n",
       "145   business  industrial output falls in japan japanese indu...   \n",
       "1210  business  us adds more jobs than expected the us economy...   \n",
       "577      sport  hearts of oak 3-2 cotonsport hearts of oak set...   \n",
       "810      sport  ronaldo considering new contract manchester un...   \n",
       "558      sport  mcilroy continues winning streak james mcilroy...   \n",
       "\n",
       "                                                NoPunct  \n",
       "2187  jones files conte lawsuit marion jones has fil...  \n",
       "1202  what now for british tennis  tim henman s deci...  \n",
       "868   moody joins up with england lewis moody has fl...  \n",
       "1339  latin america sees strong growth latin america...  \n",
       "2077  holmes starts 2005 with gb events kelly holmes...  \n",
       "...                                                 ...  \n",
       "145   industrial output falls in japan japanese indu...  \n",
       "1210  us adds more jobs than expected the us economy...  \n",
       "577   hearts of oak 32 cotonsport hearts of oak set ...  \n",
       "810   ronaldo considering new contract manchester un...  \n",
       "558   mcilroy continues winning streak james mcilroy...  \n",
       "\n",
       "[408 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Clear the punctuations\n",
    "sample_portion[\"NoPunct\"] = sample_portion['text'].apply(lambda a: re.sub(r'[^\\w\\s]','',a))\n",
    "\n",
    "display(sample_portion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4932cb5b",
   "metadata": {
    "id": "4932cb5b"
   },
   "source": [
    "### Generate the unigrams\n",
    "\n",
    "Here, we first take the bigrams, and then split into X and Y. The idea is to create a set that says \"Y occurs after X\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a32a15",
   "metadata": {
    "id": "07a32a15",
    "outputId": "12644df9-ee91-49f7-9be3-d5c913325bfc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jones\n",
      "files\n"
     ]
    }
   ],
   "source": [
    "bigram_list = []\n",
    "for line in sample_portion[\"NoPunct\"]:\n",
    "    token = word_tokenize(line)\n",
    "    bigram = list(ngrams(token, 2))\n",
    "    bigram_list.extend(bigram)\n",
    "#print(bigram_list)\n",
    "\n",
    "word_X = []\n",
    "word_Y = []\n",
    "for wordset in bigram_list:\n",
    "    word_X.append(wordset[0])\n",
    "    word_Y.append(wordset[1])\n",
    "\n",
    "print(word_X[0])\n",
    "print(word_Y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0352ef",
   "metadata": {
    "id": "ce0352ef"
   },
   "source": [
    "### Constructing the model\n",
    "\n",
    "For this, the idea is to try to predict the next word given X. For this, we have the set of word(X) and next word(Y). We will treat it as a classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59acb79e",
   "metadata": {
    "id": "59acb79e"
   },
   "outputs": [],
   "source": [
    "X_trainwv, X_testwv, y_trainwv, y_testwv = train_test_split(word_X, word_Y, random_state=10, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc51046",
   "metadata": {
    "id": "3fc51046",
    "outputId": "f6c7a519-e06a-4203-e6d2-a5351ef4e359"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>000m</th>\n",
       "      <th>000seat</th>\n",
       "      <th>000strong</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>0227</th>\n",
       "      <th>027</th>\n",
       "      <th>...</th>\n",
       "      <th>zephyr</th>\n",
       "      <th>zero</th>\n",
       "      <th>zeros</th>\n",
       "      <th>zheng</th>\n",
       "      <th>zib</th>\n",
       "      <th>zimbabwe</th>\n",
       "      <th>zinc</th>\n",
       "      <th>zogbia</th>\n",
       "      <th>zone</th>\n",
       "      <th>zurich</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109213</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109214</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109215</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109216</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109217</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>109218 rows × 11257 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0     00    000   000m  000seat  000strong     01     02   0227   \n",
       "0       False  False  False  False    False      False  False  False  False  \\\n",
       "1       False  False  False  False    False      False  False  False  False   \n",
       "2       False  False  False  False    False      False  False  False  False   \n",
       "3       False  False  False  False    False      False  False  False  False   \n",
       "4       False  False  False  False    False      False  False  False  False   \n",
       "...       ...    ...    ...    ...      ...        ...    ...    ...    ...   \n",
       "109213  False  False  False  False    False      False  False  False  False   \n",
       "109214  False  False  False  False    False      False  False  False  False   \n",
       "109215  False  False  False  False    False      False  False  False  False   \n",
       "109216  False  False  False  False    False      False  False  False  False   \n",
       "109217  False  False  False  False    False      False  False  False  False   \n",
       "\n",
       "          027  ...  zephyr   zero  zeros  zheng    zib  zimbabwe   zinc   \n",
       "0       False  ...   False  False  False  False  False     False  False  \\\n",
       "1       False  ...   False  False  False  False  False     False  False   \n",
       "2       False  ...   False  False  False  False  False     False  False   \n",
       "3       False  ...   False  False  False  False  False     False  False   \n",
       "4       False  ...   False  False  False  False  False     False  False   \n",
       "...       ...  ...     ...    ...    ...    ...    ...       ...    ...   \n",
       "109213  False  ...   False  False  False  False  False     False  False   \n",
       "109214  False  ...   False  False  False  False  False     False  False   \n",
       "109215  False  ...   False  False  False  False  False     False  False   \n",
       "109216  False  ...   False  False  False  False  False     False  False   \n",
       "109217  False  ...   False  False  False  False  False     False  False   \n",
       "\n",
       "        zogbia   zone  zurich  \n",
       "0        False  False   False  \n",
       "1        False  False   False  \n",
       "2        False  False   False  \n",
       "3        False  False   False  \n",
       "4        False  False   False  \n",
       "...        ...    ...     ...  \n",
       "109213   False  False   False  \n",
       "109214   False  False   False  \n",
       "109215   False  False   False  \n",
       "109216   False  False   False  \n",
       "109217   False  False   False  \n",
       "\n",
       "[109218 rows x 11257 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "oh = OneHotEncoder()\n",
    "word_Y_le = pd.get_dummies(y_trainwv)\n",
    "display(word_Y_le)\n",
    "print(type(word_Y_le))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413e963c",
   "metadata": {
    "id": "413e963c"
   },
   "outputs": [],
   "source": [
    "maxlen = 400\n",
    "maxlen = 400\n",
    "embedding_dims = 16\n",
    "epochs = 5\n",
    "\n",
    "max_features = 70000\n",
    "\n",
    "category_no = 20\n",
    "\n",
    "vectorizer = tf.keras.layers.TextVectorization(\n",
    "    standardize=\"lower_and_strip_punctuation\",\n",
    "    split=\"whitespace\",\n",
    "    output_mode=\"int\",\n",
    ")\n",
    "vectorizer.adapt(X_trainwv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d65692d",
   "metadata": {
    "id": "5d65692d"
   },
   "outputs": [],
   "source": [
    "gen_model = Sequential()\n",
    "\n",
    "gen_model.add(vectorizer)\n",
    "gen_model.add(Embedding(max_features, embedding_dims))\n",
    "gen_model.add(LSTM(embedding_dims, return_sequences=True, dropout=0.0, recurrent_dropout=0.1))\n",
    "gen_model.add(LSTM(embedding_dims, return_sequences=False, dropout=0.0, recurrent_dropout=0.1))\n",
    "gen_model.add(Dense(400,activation=\"relu\"))\n",
    "gen_model.add(Dense(400,activation=\"relu\"))\n",
    "gen_model.add(Dense(400,activation=\"relu\"))\n",
    "gen_model.add(Dense(11257,activation=tf.keras.activations.softmax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27826584",
   "metadata": {
    "id": "27826584",
    "outputId": "1c347bcd-af02-4e15-c108-60b57501613a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization_4 (TextV  (None, None)             0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding_6 (Embedding)     (None, None, 16)          1120000   \n",
      "                                                                 \n",
      " lstm_12 (LSTM)              (None, None, 16)          2112      \n",
      "                                                                 \n",
      " lstm_13 (LSTM)              (None, 16)                2112      \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 400)               6800      \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 400)               160400    \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 400)               160400    \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 11257)             4514057   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,965,881\n",
      "Trainable params: 5,965,881\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "3414/3414 [==============================] - 163s 47ms/step - loss: 7.1698 - accuracy: 0.0618\n",
      "Epoch 2/5\n",
      "3414/3414 [==============================] - 159s 47ms/step - loss: 6.8173 - accuracy: 0.0718\n",
      "Epoch 3/5\n",
      "3414/3414 [==============================] - 159s 47ms/step - loss: 6.6343 - accuracy: 0.0773\n",
      "Epoch 4/5\n",
      "3414/3414 [==============================] - 160s 47ms/step - loss: 6.5020 - accuracy: 0.0835\n",
      "Epoch 5/5\n",
      "3414/3414 [==============================] - 159s 47ms/step - loss: 6.3922 - accuracy: 0.0882\n"
     ]
    }
   ],
   "source": [
    "gen_model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "gen_model.summary()\n",
    "\n",
    "gen_hist = gen_model.fit(np.array(X_trainwv),word_Y_le,\n",
    "                   batch_size=32,\n",
    "                    epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1435c2",
   "metadata": {
    "id": "0a1435c2"
   },
   "source": [
    "### Generating a sentence from the trained model\n",
    "\n",
    "Just taking a random word from the excel to pass as a seed word. When we receive a prediction for the next word, the seed word is replaced by the prediction and so on until the word count is reached. We will string together the predictions for the final sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f436690",
   "metadata": {
    "id": "9f436690",
    "outputId": "02ff0690-b587-4ebc-9f95-bddd99299e19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 34ms/step\n",
      "the\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "year\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "in\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "the\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "year\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "in\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "the\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "year\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "in\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "the\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "year\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "in\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "the\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "year\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "in\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "the\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "year\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "in\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "the\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "year\n",
      "The generated sentence is:\n",
      "Face the year in the year in the year in the year in the year in the year in the year\n"
     ]
    }
   ],
   "source": [
    "seed_word = [\"Face\"]\n",
    "final_word_list = [\"Face\"]\n",
    "column_headers = list(word_Y_le.columns.values)\n",
    "\n",
    "for i in range(0,20):\n",
    "    pred = gen_model.predict(seed_word)\n",
    "    word_index = np.argmax(pred, axis=1)\n",
    "    #word = str(word_Y_le.columns[word_index].value)\n",
    "    word = column_headers[int(word_index)]\n",
    "    print(word)\n",
    "    final_word_list.append(word)\n",
    "    temp_list = []\n",
    "    temp_list.append(word)\n",
    "    seed_word = temp_list\n",
    "\n",
    "#print(final_word_list)\n",
    "\n",
    "print(\"The generated sentence is:\")\n",
    "print(\" \".join(final_word_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9113bf64",
   "metadata": {
    "id": "9113bf64"
   },
   "source": [
    "### Using pre-built MLE model\n",
    "\n",
    "In this section, we will also run the MLE model for character estimation just to compare the sentences produced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f31b85",
   "metadata": {
    "id": "62f31b85"
   },
   "outputs": [],
   "source": [
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "train, vocab = padded_everygram_pipeline(3, sample_portion[\"NoPunct\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219799c8",
   "metadata": {
    "id": "219799c8"
   },
   "outputs": [],
   "source": [
    "from nltk.lm import MLE\n",
    "model2 = MLE(3) \n",
    "model2.fit(train, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f63c4c",
   "metadata": {
    "id": "77f63c4c",
    "outputId": "0a5cef5e-720e-4146-c017-e5569e074a90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ut asking of ists ge  inding  ecoxx togt\n"
     ]
    }
   ],
   "source": [
    "word_list = model2.generate(40, random_seed=2)\n",
    "print(\"\".join(word_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af18269",
   "metadata": {
    "id": "5af18269"
   },
   "source": [
    "### Saving the custom model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f17157",
   "metadata": {
    "id": "d1f17157",
    "outputId": "b432fec5-101a-4303-9e4b-bff51faf0329"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model/gen_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model/gen_model\\assets\n"
     ]
    }
   ],
   "source": [
    "gen_model.save('saved_model/gen_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becce2d7",
   "metadata": {
    "id": "becce2d7"
   },
   "source": [
    "### Saving the encoding set for generating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fae4e5d",
   "metadata": {
    "id": "4fae4e5d",
    "outputId": "61db8b5a-3acc-4a15-93a6-5a734b5a2d83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(word_Y_le.columns.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8e0c0c",
   "metadata": {
    "id": "ad8e0c0c",
    "outputId": "e26b74e5-bed0-4353-b7f4-9d3d2fc9ceb9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>columns</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000seat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11252</th>\n",
       "      <td>zimbabwe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11253</th>\n",
       "      <td>zinc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11254</th>\n",
       "      <td>zogbia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11255</th>\n",
       "      <td>zone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11256</th>\n",
       "      <td>zurich</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11257 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        columns\n",
       "0             0\n",
       "1            00\n",
       "2           000\n",
       "3          000m\n",
       "4       000seat\n",
       "...         ...\n",
       "11252  zimbabwe\n",
       "11253      zinc\n",
       "11254    zogbia\n",
       "11255      zone\n",
       "11256    zurich\n",
       "\n",
       "[11257 rows x 1 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encodingcol = pd.DataFrame({\n",
    "    'columns': word_Y_le.columns.values\n",
    "})\n",
    "display(encodingcol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9f1a2d",
   "metadata": {
    "id": "9e9f1a2d"
   },
   "outputs": [],
   "source": [
    "encodingcol.to_csv(\"encodings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922fbefd",
   "metadata": {
    "id": "922fbefd"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
